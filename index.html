<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Delong Li | Researcher</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&family=JetBrains+Mono:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <style>
        :root {
            --bg-color: #0f172a;
            --card-bg: #1e293b;
            --text-main: #f8fafc;
            --text-secondary: #94a3b8;
            --accent: #38bdf8;
            --accent-glow: rgba(56, 189, 248, 0.3);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', sans-serif;
            background-color: var(--bg-color);
            color: var(--text-main);
            line-height: 1.6;
            overflow-x: hidden;
        }

        /* 导航栏 */
        nav {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 1.5rem 10%;
            position: fixed;
            width: 100%;
            top: 0;
            background: rgba(15, 23, 42, 0.9);
            backdrop-filter: blur(10px);
            z-index: 1000;
            border-bottom: 1px solid rgba(255,255,255,0.05);
        }

        .logo {
            font-family: 'JetBrains Mono', monospace;
            font-weight: 700;
            font-size: 1.2rem;
            color: var(--accent);
        }

        .nav-links a {
            color: var(--text-secondary);
            text-decoration: none;
            margin-left: 2rem;
            font-size: 0.9rem;
            transition: color 0.3s;
        }

        .nav-links a:hover {
            color: var(--accent);
        }

        /* Hero 区域 */
        .hero {
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: flex-start;
            padding: 0 10%;
            position: relative;
            padding-top: 80px;
        }

        .hero h1 {
            font-size: 4rem;
            font-weight: 700;
            margin-bottom: 1rem;
            background: linear-gradient(90deg, #fff, #94a3b8);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            line-height: 1.1;
        }

        .hero h2 {
            font-size: 1.5rem;
            color: var(--accent);
            margin-bottom: 1.5rem;
            font-family: 'JetBrains Mono', monospace;
        }

        .hero p {
            max-width: 650px;
            color: var(--text-secondary);
            margin-bottom: 2rem;
            font-size: 1.1rem;
        }

        /* 正文中的高亮链接样式 */
        .highlight-link {
            color: var(--accent);
            text-decoration: none;
            border-bottom: 1px solid transparent;
            transition: all 0.2s ease-in-out;
            font-weight: 600;
        }

        .highlight-link:hover {
            border-bottom: 1px solid var(--accent);
            opacity: 0.8;
        }

        .btn {
            display: inline-block;
            padding: 0.8rem 2rem;
            background: transparent;
            border: 1px solid var(--accent);
            color: var(--accent);
            text-decoration: none;
            border-radius: 5px;
            font-family: 'JetBrains Mono', monospace;
            transition: all 0.3s;
        }

        .btn:hover {
            background: var(--accent-glow);
            transform: translateY(-2px);
        }

        /* 动态打字机光标 */
        .typing-cursor::after {
            content: '|';
            animation: blink 1s step-end infinite;
        }

        @keyframes blink {
            0%, 100% { opacity: 1; }
            50% { opacity: 0; }
        }

        /* Section 通用样式 */
        section {
            padding: 5rem 10%;
        }

        .section-title {
            font-size: 2rem;
            margin-bottom: 3rem;
            display: flex;
            align-items: center;
        }

        .section-title::after {
            content: "";
            display: block;
            width: 300px;
            height: 1px;
            background: #334155;
            margin-left: 20px;
        }

        .section-title span {
            color: var(--accent);
            font-family: 'JetBrains Mono', monospace;
            font-size: 1.2rem;
            margin-right: 10px;
        }

        /* 研究兴趣标签 */
        .interests-grid {
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
            margin-bottom: 2rem;
        }

        .interest-tag {
            background: rgba(56, 189, 248, 0.1);
            color: var(--accent);
            padding: 0.5rem 1rem;
            border-radius: 50px;
            font-size: 0.9rem;
            border: 1px solid rgba(56, 189, 248, 0.2);
        }

        /* --- 论文卡片 --- */
        .paper-card {
            background: var(--card-bg);
            border-radius: 10px;
            margin-bottom: 1.5rem;
            border: 1px solid rgba(255,255,255,0.05);
            overflow: hidden;
            transition: all 0.3s ease;
        }

        .paper-card:hover {
             border-color: var(--accent);
             box-shadow: 0 10px 30px -10px rgba(0,0,0,0.5);
        }
        
        /* 卡片头部 */
        .paper-header {
            padding: 2rem;
            cursor: pointer;
            position: relative;
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
        }

        .paper-header-content {
            flex: 1;
            padding-right: 2rem;
            /* 关键修复：防止Flex子项撑开父容器，强制允许文字换行 */
            min-width: 0; 
        }

        .status-badge {
            display: inline-block;
            font-size: 0.75rem;
            color: #fbbf24;
            font-family: 'JetBrains Mono', monospace;
            border: 1px solid #fbbf24;
            padding: 0.2rem 0.6rem;
            border-radius: 4px;
            margin-bottom: 1rem;
        }

        .paper-title {
            font-size: 1.3rem;
            font-weight: 600;
            margin-bottom: 0.5rem;
            color: var(--text-main);
            transition: color 0.3s;
            /* 关键修复：强制长单词换行，避免重叠 */
            overflow-wrap: break-word;
            word-wrap: break-word;
            word-break: break-word; 
            hyphens: auto;
        }
        
        .paper-card:hover .paper-title {
            color: var(--accent);
        }

        .paper-meta {
            font-size: 0.9rem;
            color: var(--text-secondary);
            margin-bottom: 1rem;
        }

        .paper-short-desc {
            color: var(--text-secondary); 
            font-size: 0.95rem;
            /* 同样应用换行规则 */
            overflow-wrap: break-word;
        }

        .toggle-icon {
            color: var(--accent);
            font-size: 1.2rem;
            transition: transform 0.4s ease;
            margin-top: 1rem; 
            /* 防止箭头被挤压 */
            flex-shrink: 0; 
        }

        .paper-card.open .toggle-icon {
            transform: rotate(180deg);
        }

        .paper-body {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.5s ease-in-out, opacity 0.4s ease-in-out;
            opacity: 0;
            background: rgba(15, 23, 42, 0.3);
        }

        .paper-card.open .paper-body {
            max-height: 1500px; /* 增加高度限制以适应长摘要 */
            opacity: 1;
            border-top: 1px solid rgba(255,255,255,0.05);
        }

        .paper-abstract {
            padding: 2rem;
            color: var(--text-secondary);
            font-size: 0.95rem;
            line-height: 1.7;
            /* 确保摘要内容也会正确换行 */
            overflow-wrap: break-word; 
        }
        
        .paper-abstract strong {
            color: var(--accent);
            display: block;
            margin-bottom: 0.5rem;
            font-family: 'JetBrains Mono', monospace;
        }

        /* Footer */
        footer {
            text-align: center;
            padding: 2rem;
            color: var(--text-secondary);
            font-size: 0.8rem;
            border-top: 1px solid rgba(255,255,255,0.05);
        }

        /* 移动端适配 */
        @media (max-width: 768px) {
            .hero h1 { font-size: 2.5rem; word-break: break-word; }
            .nav-links { display: none; }
            nav, .hero, section { padding: 1.5rem 5%; }
            .section-title::after { width: 100px; }
            .paper-header { padding: 1.5rem; }
            .paper-abstract { padding: 1.5rem; }
            /* 移动端稍微减小标题字号，防止过于拥挤 */
            .paper-title { font-size: 1.1rem; }
        }
    </style>
</head>
<body>

    <nav>
        <div class="logo">DL.</div>
        <div class="nav-links">
            <a href="#about">About</a>
            <a href="#research">Research</a>
            <a href="#contact">Contact</a>
        </div>
    </nav>

    <section class="hero">
        <h2 class="typing-cursor">Hi, I am Delong Li</h2>
        <h1>Building Trustworthy AI</h1>
        <p>
            Master by Research at <a href="https://www.uts.edu.au" target="_blank" class="highlight-link">University of Technology Sydney (UTS)</a>.<br>
            Research Support at <a href="https://www.uts.edu.au/about/faculties/engineering-and-information-technology/electrical-and-data-engineering" target="_blank" class="highlight-link">School of Electrical & Data Engineering</a>.
            <br>
            Member of <a href="https://utscyber.com" target="_blank" class="highlight-link">CyberR Lab</a>.
        </p>
        <p style="font-size: 0.9rem; color: var(--accent); margin-top: -1rem;">
            Focusing on Machine Unlearning, AI Security, Federated Learning.
        </p>
        <a href="#contact" class="btn">Get In Touch</a>
    </section>

    <section id="about">
        <h3 class="section-title"><span>01.</span>About Me</h3>
        <div style="max-width: 800px;">
            <p style="margin-bottom: 1.5rem; color: var(--text-secondary);">
                I am currently a Master by Research student at the <strong>University of Technology Sydney (UTS)</strong>, serving as Research Support within the <strong>School of Electrical & Data Engineering</strong>. I am also proud to be a member of the <a href="https://utscyber.com" target="_blank" class="highlight-link">CyberR Lab</a>.
            </p>
            <p style="margin-bottom: 2rem; color: var(--text-secondary);">
                My research sits at the intersection of privacy, security, and large language models. I am passionate about making AI systems not only more powerful but also more <strong>auditable, secure, and compliant</strong> with human values.
            </p>
            
            <h4 style="margin-bottom: 1rem; color: var(--text-main);">Research Interests</h4>
            <div class="interests-grid">
                <span class="interest-tag">Machine Unlearning</span>
                <span class="interest-tag">AI for Security (AI 4 Security)</span>
                <span class="interest-tag">Federated Learning</span>
                <span class="interest-tag">Split Learning</span>
                <span class="interest-tag">Large Language Models (LLMs)</span>
            </div>
        </div>
    </section>

    <section id="research">
        <h3 class="section-title"><span>02.</span>Selected Works</h3>
        <p style="margin-bottom: 2rem; color: var(--text-secondary);">Click on a paper card to view its abstract.</p>
        
        <div class="paper-card">
            <div class="paper-header">
                <div class="paper-header-content">
                    <div class="status-badge">In Process</div>
                    <div class="paper-title">AuditableLLM: A Hash-Chain-Backed, Compliance-Aware Auditable Framework for Large Language Models</div>
                    <div class="paper-meta">
                        <strong>Delong Li</strong>, <strong>Guangsheng Yu</strong>, <strong>Xu Wang</strong>, <strong>Bin Liang</strong>
                    </div>
                    <p class="paper-short-desc">
                        Proposing a novel framework to ensure compliance and auditability in LLMs using hash-chain technology.
                    </p>
                </div>
                <i class="fas fa-chevron-down toggle-icon"></i>
            </div>
            <div class="paper-body">
                <div class="paper-abstract">
                    <strong>Abstract:</strong>
                    Ensuring auditability, transparency, and regulatory compliance in large language models (LLMs) has become a key requirement for trustworthy AI deployment. Existing research often focuses on isolated stages such as training or unlearning, lacking a unified, algorithm-level mechanism for verifiable accountability. In this work, we present AuditableLLM, a lightweight audit framework that decouples model update operations from the audit and verification layer, and is designed to augment a broad class of learning or unlearning processes with a hash-chain-backed, tamper-evident trail. The framework is compatible with diverse adaptation regimes, including parameter-efficient fine-tuning (e.g., Low-Rank Adaptation (LoRA) / QLoRA), full-parameter optimization, continual learning, and data unlearning, and enables third-party verification without access to model internals or raw logs. We evaluate AuditableLLM on representative LLM adaptation and unlearning tasks using the LLaMA family of models with LoRA adapters and the MovieLens dataset. Experimental results demonstrate that AuditableLLM preserves task utility (with less than 0.2% deviation in accuracy and macro-F1), while introducing only a small computational overhead (approximately 3.4 ms per step, or 5.7% slowdown) and achieving essentially perfect verification integrity with sub-second audit validation in our setting, and that, under a simple loss-based membership inference attack on the forget set, the audit layer does not increase membership leakage relative to the underlying unlearning algorithm. Taken together, these results show that a lightweight, hash-chain-backed audit layer can provide tamper-evident logging for LoRA-based LLM adaptation under an honest-but-curious provider with modest overhead, and can support compliance-aware workflows without amplifying membership leakage in our experimental setting.
                </div>
            </div>
        </div>

        <div class="paper-card">
            <div class="paper-header">
                <div class="paper-header-content">
                    <div class="status-badge">In Process</div>
                    <div class="paper-title">A Survey of LoRA-based Machine Unlearning for LLMs: Methods, Taxonomy, and Evaluation</div>
                    <div class="paper-meta">
                        <strong>Delong Li</strong>, <strong>Guangsheng Yu</strong>, <strong>Xu Wang</strong>, <strong>Yanna Jiang</strong>, <strong>Wencheng Yang</strong>, <strong>Bin Liang</strong>, <strong>Wei Ni</strong>
                    </div>
                    <p class="paper-short-desc">
                         A comprehensive survey categorizing current methodologies in parameter-efficient unlearning with taxonomy and evaluation standards.
                    </p>
                </div>
                <i class="fas fa-chevron-down toggle-icon"></i>
            </div>
            <div class="paper-body">
                <div class="paper-abstract">
                    <strong>Abstract:</strong>
                      Machine unlearning is becoming a practical requirement for deploying large language models (LLMs) under privacy, safety, and compliance constraints. In parallel, modern adaptation pipelines increasingly rely on parameter-efficient fine-tuning (PEFT), where a frozen backbone is paired with lightweight trainable modules such as Low-Rank Adaptation (LoRA). This survey reviews LoRA-based machine unlearning for LLMs, focusing on adapter-centric settings in which forgetting is achieved by operating primarily in PEFT module space rather than updating the full model. We formalize the LoRA unlearning problem under a frozen base model and present a taxonomy of existing methods, including gradient-based objectives, influence- and Fisher-informed updates, structural and continual-unlearning schemes, bounded-dynamics approaches, and pruning-enhanced variants. To relate these families, we introduce a unified adapter-space view that expresses seemingly disparate techniques as instances of a common update operator over adapter parameters. We further summarize evaluation protocols, metrics, and practical assumptions used in prior work, and provide a worked Task Of Fictitious Unlearning (TOFU) case study to illustrate utility–forgetting trade-offs across representative categories. Finally, we highlight open challenges such as repeated deletion requests, constrained data-access settings, federated or split-adapter deployments, and integration with auditing and compliance workflows.
                </div>
            </div>
        </div>

        <div class="paper-card">
            <div class="paper-header">
                 <div class="paper-header-content">
                    <div class="status-badge">In Process</div>
                    <div class="paper-title">FedVILA: Federated LoRA-based Unlearning for Large Language Models</div>
                    <div class="paper-meta">
                        <strong>Delong Li</strong>, <strong>Guangsheng Yu</strong>, <strong>Xu Wang</strong>, <strong>Bin Liang</strong> et al.
                    </div>
                    <p class="paper-short-desc">
                        Exploring the intersection of Federated Learning and Machine Unlearning utilizing LoRA adapters for efficient data removal.
                    </p>
                </div>
                <i class="fas fa-chevron-down toggle-icon"></i>
            </div>
            <div class="paper-body">
                <div class="paper-abstract">
                    <strong>Abstract:</strong>
                    Large language models (LLMs) trained on decentralized data create a need for federated machine unlearning, where the influence of specific clients or authors is removed without retraining from scratch. Existing benchmarks such as TOFU target centralized author-level unlearning and do not address federated optimization or parameter-efficient adaptation. We introduce FedVILA, a LoRA-based federated unlearning framework that combines TOFU-style evaluation with importance-guided parameter updates. In our setup, a small subset of TOFU authors is assigned to a forget client, while the remaining authors are distributed over retain clients. Starting from a Llama 3.2 1B model trained via FedAvg with LoRA on all clients, FedVILA runs a federated unlearning phase where the forget client optimizes an Inverted Hinge Loss and retain clients continue language modeling. Using gradient-based importance to define a low-rank LoRA subspace, we report TOFU Truth Ratio and KS metrics, showing improved federated forgetting with largely preserved utility.
                </div>
            </div>
        </div>

    </section>

    <section id="contact" style="text-align: center; padding-bottom: 10rem;">
        <h3 class="section-title" style="justify-content: center; margin-bottom: 1rem;"><span>03.</span>What's Next?</h3>
        <h2 style="font-size: 3rem; margin-bottom: 1.5rem;">Get In Touch</h2>
        <p style="max-width: 600px; margin: 0 auto 3rem auto; color: var(--text-secondary);">
            I am always open to discussing new research collaborations, especially regarding LLM security and privacy. Whether you have a question or just want to say hi, my inbox is open!
        </p>
        <a href="mailto:delong.li-1@student.uts.edu.au" class="btn">Say Hello</a>
        
        <div style="margin-top: 3rem; font-size: 1.5rem;">
            <a href="https://github.com/DELONG-L" target="_blank" style="color: var(--text-secondary); margin: 0 10px; transition: color 0.3s;" onmouseover="this.style.color='#38bdf8'" onmouseout="this.style.color='#94a3b8'"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/delong-li-144941353" target="_blank" style="color: var(--text-secondary); margin: 0 10px; transition: color 0.3s;" onmouseover="this.style.color='#38bdf8'" onmouseout="this.style.color='#94a3b8'"><i class="fab fa-linkedin"></i></a>
            <a href="https://orcid.org/0009-0000-8648-8921" style="color: var(--text-secondary); margin: 0 10px; transition: color 0.3s;" onmouseover="this.style.color='#38bdf8'" onmouseout="this.style.color='#94a3b8'"><i class="fas fa-graduation-cap"></i></a>
        </div>
    </section>

    <footer>
        <p>Designed & Built by Delong Li.</p>
        <p>&copy; 2025 All rights reserved.</p>
    </footer>

    <script>
        // 简单的打字机效果脚本
        const text = "Hi, I am Delong Li";
        const typingElement = document.querySelector('.typing-cursor');
        let index = 0;

        function type() {
            if (index < text.length) {
                typingElement.textContent = text.slice(0, index + 1);
                index++;
                setTimeout(type, 100);
            }
        }
        
        // 页面加载完成后开始打字
        window.onload = () => {
            typingElement.textContent = "";
            setTimeout(type, 500);
        };

        // Accordion functionality for papers
        document.querySelectorAll('.paper-header').forEach(header => {
            header.addEventListener('click', () => {
                const card = header.parentElement;
                card.classList.toggle('open');
            });
        });
    </script>
</body>
</html>
